# Multi-Process Sandbox Architecture

In general, browsers have one main process that runs the UI and manages all the other processes - this is known as the "browser process" or "browser" for short. Very unique, I know. The processes that handle the web content are known as the "renderer processes" or "renderers". These render processes utilize something called Blink which is the open-source rendering engine used by Chrome. Blink implements many other libraries that help it run, such as Skia, which is an open-source 2D graphics library, and of course V8 for JavaScript.

Now, here are where things get a little bit complicated. In Chrome, each new window or tab opens up in a new process - which usually will be a new render process. This new render process has a global RenderProcess object that manages communication with the parent browser process and maintains global state of the web page or application within that window or tab. In turn, the main browser process will maintain a corresponding RenderProcessHost object for each renderer, which manages browser state and communication for the renderer.

To communicate between each of these processes, Chromium uses either a legacy IPC system or Mojo.

Reference: https://www.chromium.org/developers/design-documents/multi-process-architecture/

# Sandbox

Sandbox operates at process-level granularity. Anything that needs to be sandboxed needs to live on a separate process. The minimal sandbox configuration has two processes: one that is a privileged controller known as the broker, and one or more sandboxed processes known as the target. Throughout the documentation and the code these two terms are used with that precise connotation. The sandbox is provided as a static library that must be linked to both the broker and the target executables.

In general, this limits what an attacker can do if they obtain remote code execution within a renderer process. Essentially, they won’t be able to make persistent changes to the computer or access information such as user input and cookies in other windows and tabs without exploiting or chaining another bug to break out of that sandbox.

## The broker process

In Chromium, the broker is always the browser process. The broker, is in broad terms, a privileged controller/supervisor of the activities of the sandboxed processes. The responsibilities of the broker process are:
- Specify the policy for each target process
- Spawn the target processes
- Host the sandbox policy engine service
- Host the sandbox interception manager
- Host the sandbox IPC service (to the target processes)
- Perform the policy-allowed actions on behalf of the target process

The broker should always outlive all the target processes that it spawned. The sandbox IPC is a low-level mechanism (different from Chromium's IPC) that is used to transparently forward certain Windows API calls from the target to the broker: these calls are evaluated against the policy. The policy-allowed calls are then executed by the broker and the results returned to the target process via the same IPC. The job of the interceptions manager is to patch the Windows API calls that should be forwarded via IPC to the broker.

## The target process

In Chromium, the renderers are always target processes, unless the --no-sandbox command line has been specified for the browser process. The target process hosts all the code that is going to run inside the sandbox, plus the sandbox infrastructure client side:
- All code to be sandboxed
- The sandbox IPC client
- The sandbox policy engine client
- The sandbox interceptions

Items 2,3 and 4 are part of the sandbox library that is linked with the code to be sandboxed.

The interceptions (also known as hooks) are how Windows API calls are forwarded via the sandbox IPC to the broker. It is up to the broker to re-issue the API calls and return the results or simply fail the calls. The interception + IPC mechanism does not provide security; it is designed to provide compatibility when code inside the sandbox cannot be modified to cope with sandbox restrictions. To save unnecessary IPCs, policy is also evaluated in the target process before making an IPC call, although this is not used as a security guarantee but merely a speed optimization.

Reference: https://chromium.googlesource.com/chromium/src/+/HEAD/docs/design/sandbox.md

# Blink

## Processes

How many renderer processes are created? For security reasons, it is important to isolate memory address regions between cross-site documents (this is called Site Isolation). Conceptually each renderer process should be dedicated to at most one site. Realistically, however, it's sometimes too heavy to limit each renderer process to a single site when users open too many tabs or the device does not have enough RAM. Then a renderer process may be shared by multiple iframes or tabs loaded from different sites. This means that iframes in one tab may be hosted by different renderer processes and that iframes in different tabs may be hosted by the same renderer process. There is no 1:1 mapping between renderer processes, iframes and tabs.

Given that a renderer process runs in a sandbox, Blink needs to ask the browser process to dispatch system calls (e.g., file access, play audio) and access user profile data (e.g., cookie, passwords). This browser-renderer process communication is realized by Mojo. (Note: In the past we were using Chromium IPC and a bunch of places are still using it. However, it's deprecated and uses Mojo under the hood.) On the Chromium side, Servicification is ongoing and abstracting the browser process as a set of "service"s. From the Blink perspective, Blink can just use Mojo to interact with the services and the browser process.

## Threads

Blink has one main thread, N worker threads and a couple of internal threads.

Almost all important things happen on the main thread. All JavaScript (except workers), DOM, CSS, style and layout calculations run on the main thread. Blink is highly optimized to maximize the performance of the main thread, assuming the mostly single-threaded architecture.

Blink may create multiple worker threads to run Web Workers, ServiceWorker and Worklets.

Blink and V8 may create a couple of internal threads to handle webaudio, database, GC etc.

SOURCE: https://docs.google.com/document/u/0/d/1aitSOucL0VHZa9Z2vbRJSyAIsAz24kX8LFByQ5xQnUg/mobilebasic

Within the documentation it states that Blink runs in each renderer process and it has one main thread which handles JavaScript, DOM, CSS, style and layout calculations. Additionally, Blink can also create multiple "worker" threads to run additional scripts, extensions, etc.

In general, each Blink thread runs its own instance of V8. Why? Well as you know, within a separate browser window or tab there can be a lot of JavaScript code running, not just for the page, but in different iframes for stuff like ads, buttons, etc. At the end of the day each of those scripts and iframes have separate JavaScript contexts and there has to be a way of preventing one script from manipulating objects in another.

# V8's Isolate and Context

In V8, an Isolate is simply a concept of an instance or "virtual machine" which represents one JavaScript execution environment; including a heap manager, a garbage collector, etc. In Blink, isolates and threads have a 1:1 relationship, where one isolate is associated with the main thread and one isolate is associated with one worker thread.

Now, the Context corresponds to a global root object which holds the state of the VM and is used to compile and execute scripts in a single instance of V8. Roughly speaking, one window object corresponds to one context, and since each frame has a window object, there are potentially multiple contexts in a renderer process. In relation to the isolate, the isolate and contexts have a 1:N relationship over the lifetime of the isolate - where that specific isolate or instance will interpret and compile multiple contexts.

This means that each time JavaScript need to be executed, we need to validate that we are in the correct context via `GetCurrentContext()` or we’ll end up either leaking JavaScript objects or overwriting them, which potentially can cause a security issue.

Isolate definition from V8:

```cpp
/**
 * Isolate represents an isolated instance of the V8 engine.  V8 isolates have
 * completely separate states.  Objects from one isolate must not be used in
 * other isolates.  The embedder can create multiple isolates and use them in
 * parallel in multiple threads.  An isolate can be entered by at most one
 * thread at any given time.  The Locker/Unlocker API must be used to
 * synchronize.
 */

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/include/v8-isolate.h;l=210
```

Context definition from V8:

```cpp
/**
 * A sandboxed execution context with its own set of built-in objects
 * and functions.
 */

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/include/v8-context.h;l=48
```

# Ignition the Interpreter

## Register-Based Machine

As we know, the Ignition interpreter is a register-based interpreter with an accumulator register. These "registers" aren’t actually traditional machine registers as one would think. Instead, they are specific slots in a register file which is allocated as part of a function’s stack frame - in essence they are "virtual" registers.

### Bytecode Handlers

Ignition consists of a set of **bytecode handlers** which are written in a high-level, machine agnostic assembly code. These handlers are implemented by the [`CodeStubAssembler`](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/interpreter-generator.cc;l=38) class and compiled by using TurboFan’s backend when the browser is compiled:

Overall, each of these handlers "handles" a specific bytecode and then dispatches to the next bytecode’s respective handler.

An example of the `LdaZero` or "Load Zero to Accumulator" bytecode handler from v8/src/interpreter/interpreter-generator.cc can be seen below.

```cpp
// LdaZero
// Load literal '0' into the accumulator.
IGNITION_HANDLER(LdaZero, InterpreterAssembler) 
{
  TNode<Number> zero_value = NumberConstant(0.0);
  SetAccumulator(zero_value);
  Dispatch();
}
```

This is the "template" of an handler that create `Name##Assembler` class and a `Generate` function that will be called to generate the assembly code for the handler:

```cpp
#define IGNITION_HANDLER(Name, BaseAssembler)                         \
  class Name##Assembler : public BaseAssembler {                      \
   public:                                                            \
    explicit Name##Assembler(compiler::CodeAssemblerState* state,     \
                             Bytecode bytecode, OperandScale scale)   \
        : BaseAssembler(state, bytecode, scale) {}                    \
    Name##Assembler(const Name##Assembler&) = delete;                 \
    Name##Assembler& operator=(const Name##Assembler&) = delete;      \
    static void Generate(compiler::CodeAssemblerState* state,         \
                         OperandScale scale);                         \
                                                                      \
   private:                                                           \
    void GenerateImpl();                                              \
  };                                                                  \
  void Name##Assembler::Generate(compiler::CodeAssemblerState* state, \
                                 OperandScale scale) {                \
    Name##Assembler assembler(state, Bytecode::k##Name, scale);       \
    state->SetInitialDebugInformation(#Name, __FILE__, __LINE__);     \
    assembler.GenerateImpl();                                         \
  }                                                                   \
  void Name##Assembler::GenerateImpl()

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/interpreter-generator.cc;l=38
```

<!-- Actually, the generated assembly instructions are just functions written in C++. For example:

```cpp
void Assembler::leave() {
  EnsureSpace ensure_space(this);
  emit(0xC9);
}
// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/codegen/x64/assembler-x64.cc
```

- Where `EnsureSpace` is a helper class ensures that there is enough space in the buffer to emit the instruction.

  ```cpp
  // Helper class that ensures that there is enough space for generating
  // instructions and relocation information.  The constructor makes
  // sure that there is enough space and (in debug mode) the destructor
  // checks that we did not generate too much.
  class EnsureSpace {
  public:
    explicit V8_INLINE EnsureSpace(Assembler* assembler) : assembler_(assembler) {
      if (V8_UNLIKELY(assembler_->buffer_overflow())) assembler_->GrowBuffer();
  #ifdef DEBUG
      space_before_ = assembler_->available_space();
  #endif
    }
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/codegen/x64/assembler-x64.h;drc=90cac1911508d3d682a67c97aa62483eb712f69a;bpv=1;bpt=1;l=3122
  ```

- And `emit` is just a function that writes the instruction to the buffer which is pointed to by the `pc_` (pointer counter) member variable of the `Assembler` class:

  ```cpp
  void emit(uint8_t x) { pc_ = emit(pc_, x); }
  void emitw(uint16_t x) { pc_ = emit(pc_, x); }
  void emitl(uint32_t x) { pc_ = emit(pc_, x); }
  void emitq(uint64_t x) { pc_ = emit(pc_, x); }
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/codegen/x64/assembler-x64.h;l=2499;drc=90cac1911508d3d682a67c97aa62483eb712f69a;bpv=1;bpt=1
  ``` -->

When V8 creates a new isolate, it will load the handlers from a snapshot file that was created during build time. The isolate will also contain a global interpreter **dispatch table** which holds a code object pointer to each bytecode handler, as indexed by the bytecode value. Generally, this dispatch table is simply just an enum.

### Generating Bytecode
  
When a JavaScript function is parsed, the V8 parser will generate an Abstract Syntax Tree (AST) for the function.

In order for the bytecode to be **run by Ignition**, the JavaScript function is first translated to bytecode from its AST by a [`BytecodeGenerator`](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/bytecode-generator.h;l=48;bpv=1;bpt=1). This generator walks the AST and emits the appropriate bytecode per each AST node by calling the `GenerateBytecode` function.

This bytecode is then associated with a property field known as the `SharedFunctionInfo` of `JSFunction` object. 

- Definition of `JSFunction` from V8:

  ```cpp
  // JSFunction describes JavaScript functions.
  class JSFunction : public TorqueGeneratedJSFunction<
                        JSFunction, JSFunctionOrBoundFunctionOrWrappedFunction> {
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-function.h;l=90
  ```

- Definition of `SharedFunctionInfo` from V8:

  ```cpp
  // SharedFunctionInfo describes the JSFunction information that can be
  // shared by multiple instances of the function.
  class SharedFunctionInfo
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/shared-function-info.h;l=196?q=SharedFunctionInfo&ss=chromium%2Fchromium%2Fsrc
  ```

### Stack Frame Generation and Execution

Afterwards, the JavaScript function's [`code_entry_point`](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-function.h;l=126) is set to the [`InterpreterEntryTrampoline`](https://source.chromium.org/chromium/chromium/src/+/main:v8/src/interpreter/interpreter.cc;l=344?q=InterpreterEntryTrampoline&ss=chromium%2Fchromium%2Fsrc) **built-in stub**.

The `code_entry_point` in `JSFunction`:

```cpp
// [code]: The generated code object for this function.  Executed
// when the function is invoked, e.g. foo() or new foo(). See
// [[Call]] and [[Construct]] description in ECMA-262, section
// 8.6.2, page 27.
// Release/Acquire accessors are used when storing a newly-created
// optimized code object, or when reading from the background thread.
// Storing a builtin doesn't require release semantics because these objects
// are fully initialized.
DECL_CODE_POINTER_ACCESSORS(code)
// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-function.h;l=126
```

The `InterpreterEntryTrampoline` stub is entered when a JavaScript function **is called** (if the function is not called, the stub will be `CompileLazy` and the bytecode will not be generated), and is responsible for **setting up the appropriate interpreter stack frame** while also **dispatching to the interpreter’s bytecode handler** for the first bytecode of the function. This then starts the execution or "interpretation" of the function by Ignition.

- The part of `InterpreterEntryTrampoline` stub that load the bytecode:

  ```cpp
  // Generate code for entering a JS function with the interpreter.
  // On entry to the function the receiver and arguments have been pushed on the
  // stack left to right.
  //
  // The live registers are:
  //   o rax: actual argument count
  //   o rdi: the JS function object being called
  //   o rdx: the incoming new target or generator object
  //   o rsi: our context
  //   o rbp: the caller's frame pointer
  //   o rsp: stack pointer (pointing to return address)
  //
  // The function builds an interpreter frame. See InterpreterFrameConstants in
  // frame-constants.h for its layout.
  void Builtins::Generate_InterpreterEntryTrampoline(
      MacroAssembler* masm, InterpreterEntryTrampolineMode mode) {
    Register closure = rdi;

    // Get the bytecode array from the function object and load it into
    // kInterpreterBytecodeArrayRegister.
    const Register shared_function_info(r11);
    __ LoadTaggedField(
        shared_function_info,
        FieldOperand(closure, JSFunction::kSharedFunctionInfoOffset));
    ResetSharedFunctionInfoAge(masm, shared_function_info);

    // The bytecode array could have been flushed from the shared function info,
    // if so, call into CompileLazy.
    Label is_baseline, compile_lazy;
    GetSharedFunctionInfoBytecodeOrBaseline(
        masm, shared_function_info, kInterpreterBytecodeArrayRegister,
        kScratchRegister, &is_baseline, &compile_lazy);

  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/x64/builtins-x64.cc;l=1076?q=InterpreterEntryTrampoline&ss=chromium%2Fchromium%2Fsrc
  ```

- The part that it pushes things to stack frame:

  ```cpp
  __ bind(&push_stack_frame);
    FrameScope frame_scope(masm, StackFrame::MANUAL);
    __ pushq(rbp);  // Caller's frame pointer.
    __ movq(rbp, rsp);
    __ Push(kContextRegister);                 // Callee's context.
    __ Push(kJavaScriptCallTargetRegister);    // Callee's JS function.
    __ Push(kJavaScriptCallArgCountRegister);  // Actual argument count.

    // Load initial bytecode offset.
    __ Move(kInterpreterBytecodeOffsetRegister,
            BytecodeArray::kHeaderSize - kHeapObjectTag);

    // Push bytecode array and Smi tagged bytecode offset.
    __ Push(kInterpreterBytecodeArrayRegister);
    __ SmiTag(rcx, kInterpreterBytecodeOffsetRegister);
    __ Push(rcx);

    // Push feedback vector.
    __ Push(feedback_vector);

    // Allocate the local and temporary register file on the stack.
    Label stack_overflow;
    {
      // Load frame size from the BytecodeArray object.

  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/x64/builtins-x64.cc;l=1137?q=InterpreterEntryTrampoline&ss=chromium%2Fchromium%2Fsrc
  ```

Specifically, in builtins-x64.cc, the `Builtins::Generate_InterpreterPushArgsThenCallImpl` and `Builtins::Generate_InterpreterPushArgsThenConstructImpl` functions are responsible for further building out the interpreter stack frame by pushing the arguments and function state to the stack.

- The part that it dispatches to bytecode handler:

  ```cpp
  // Load the dispatch table into a register and dispatch to the bytecode
    // handler at the current bytecode offset.
    Label do_dispatch;
    __ bind(&do_dispatch);
    __ Move(
        kInterpreterDispatchTableRegister,
        ExternalReference::interpreter_dispatch_table_address(masm->isolate()));
    __ movzxbq(kScratchRegister,
              Operand(kInterpreterBytecodeArrayRegister,
                      kInterpreterBytecodeOffsetRegister, times_1, 0));
    __ movq(kJavaScriptCallCodeStartRegister,
            Operand(kInterpreterDispatchTableRegister, kScratchRegister,
                    times_system_pointer_size, 0));
    __ call(kJavaScriptCallCodeStartRegister);

  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/x64/builtins-x64.cc;l=1205?q=InterpreterEntryTrampoline&ss=chromium%2Fchromium%2Fsrc
  ```

> Question: What is the `InterpreterDispatchTable` stub? ANSWER: it is a built-in function that will set up the stack frame and dispatch to the bytecode handler.

During bytecode generation, the `BytecodeGenerator` will also allocate registers in a function’s register file (a set of registers used in execution of Ignition) for local variables, context object pointers, and temporary values that are required for expression evaluation.

The `InterpreterEntryTrampoline` stub handles the initial building of the stack frame, and then allocates space in the stack frame for the register file (which is allocated for data by `BytecodeGenerator`, as mentioned above). This stub will also write undefined to all the registers in this register file so that the Garbage Collector (GC) doesn’t see any invalid (i.e., non-tagged) pointers when it walks the stack:

```cpp
    // Do a stack check to ensure we don't go over the limit.
    __ movq(rax, rsp);
    __ subq(rax, rcx);
    __ cmpq(rax, __ StackLimitAsOperand(StackLimitKind::kRealStackLimit));
    __ j(below, &stack_overflow);

    // If ok, push undefined as the initial value for all register file entries.
    Label loop_header;
    Label loop_check;
    __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
    __ jmp(&loop_check, Label::kNear);
    __ bind(&loop_header);
    // TODO(rmcilroy): Consider doing more than one push per loop iteration.
    __ Push(kInterpreterAccumulatorRegister);
    // Continue loop if not done.
    __ bind(&loop_check);
    __ subq(rcx, Immediate(kSystemPointerSize));
    __ j(greater_equal, &loop_header, Label::kNear);

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/builtins/x64/builtins-x64.cc;l=1164?q=InterpreterEntryTrampoline&ss=chromium%2Fchromium%2Fsrc
```

Stack frame layout:

```txt
+---------------------------+
|        ...                |
+---------------------------+
|   Spilled registers...    |
+---------------------------+
|          Context          |
+---------------------------+
|        STUB Marker        |
+---------------------------+
|       Frame Pointer       |
+---------------------------+
| Ignition Entry Trampoline |
|           $PC             |
+---------------------------+
|           R3              |
+---------------------------+
|           R2              |
+---------------------------+
|           R1              |
+---------------------------+
|           R0              |
+---------------------------+
|     Bytecode Offset       |
+---------------------------+
|      Bytecode Array       |
+---------------------------+
|          Context          |
+---------------------------+
|        JS Function        |
+---------------------------+
|       Frame Pointer       |
+---------------------------+
|        Caller $PC         |
+---------------------------+
|           Arg1            |
+---------------------------+
|           Arg0            |
+---------------------------+
|          <this>           |
+---------------------------+
|      Caller frame...      |
+---------------------------+
|           ...             |
+---------------------------+
```

The section from Bytecode Offset to Caller $PC contains the Isolates current context object (Context), the caller pointer counter (Caller $PC), and a pointer to the `JSFunction` object. This pointer to `JSFunction` is also knowns as the **closure** which links to the functions context, `SharedFunctionInfo` object, as well as to other accessors like the `FeedbackVector`.

- The function context in `JSFunction` (should not be mistaken with the Isolate context) is a `Context` object which holds the local variables and the `this` object for the function:

  ```cpp
  // [context]: The context for this function.
  inline Tagged<Context> context();
  DECL_RELAXED_GETTER(context, Tagged<Context>)
  inline bool has_context() const;
  using TorqueGeneratedClass::context;
  using TorqueGeneratedClass::set_context;
  DECL_RELEASE_ACQUIRE_ACCESSORS(context, Tagged<Context>)
  inline Tagged<JSGlobalProxy> global_proxy();
  inline Tagged<NativeContext> native_context();
  inline int length();
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-function.h;l=104
  ```

- The feedback vector in `JSFunction` is used to store the inline caches for the function:

  ```cpp
  // [raw_feedback_cell]: Gives raw access to the FeedbackCell used to hold the
  /// FeedbackVector eventually. Generally this shouldn't be used to get the
  // feedback_vector, instead use feedback_vector() which correctly deals with
  // the JSFunction's bytecode being flushed.
  DECL_ACCESSORS(raw_feedback_cell, Tagged<FeedbackCell>)
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/objects/js-function.h;l=219
  ```

You might also notice that there is no accumulator register in the stack frame. And the reason for that is because the accumulator register will change constantly during function calls, in that case it’s kept within the Interpreter as a state register. This state register is pointed to by the **Frame Pointer** (FP), which also holds the stack pointer and frame counter.

Going back to the first stack frame example, you will also notice that there is a Bytecode Array pointer. This `BytecodeArray` represents a sequence of interpreter bytecodes for that specific function within the stack frame. Initially each bytecode is an enum where the **index of the bytecode stores the corresponding handler** - as explained previously.

- This is how the interpreter find the index of a bytecode in the **dispatch table** (as mentioned in the end of [Bytecode Handlers](#bytecode-handlers)), which is used to call the handler:

  ```cpp
  size_t Interpreter::GetDispatchTableIndex(Bytecode bytecode,
                                            OperandScale operand_scale) {
    static const size_t kEntriesPerOperandScale = 1u << kBitsPerByte;
    size_t index = static_cast<size_t>(bytecode);
    return index + BytecodeOperands::OperandScaleAsIndex(operand_scale) *
                      kEntriesPerOperandScale;
  }
  ```

- And this is how the interpreter find the corresponding handler from the `BytecodeArray`:

  ```js
  d8> var num = 42;
  [generated bytecode for function:  (0x03650025a599 <SharedFunctionInfo>)]
  Bytecode length: 18
  Parameter count 1
  Register count 3
  Frame size 24
  Bytecode age: 0
          000003650025A61E @    0 : 13 00             LdaConstant [0]
          000003650025A620 @    2 : c4                Star1
          000003650025A621 @    3 : 19 fe f8          Mov <closure>, r2
          000003650025A624 @    6 : 66 5f 01 f9 02    CallRuntime [DeclareGlobals], r1-r2
          000003650025A629 @   11 : 0d 2a             LdaSmi [42]
          000003650025A62B @   13 : 23 01 00          StaGlobal [1], [0]
          000003650025A62E @   16 : 0e                LdaUndefined
          000003650025A62F @   17 : aa                Return
  ```

  Take a look at the 1st line in the bytecode array, it stores `LdaConstant`. To the left of it we see `13 00`. The hex number `0x13` is the bytecode enumerator for `LdaConstant`, which represents where the handler for that bytecode will be.

- Once that’s received, the `SetBytecodeHandler()` will be called with the bytecode, operands, and it’s handlers enum:

  ```cpp
  void Interpreter::SetBytecodeHandler(Bytecode bytecode,
                                      OperandScale operand_scale,
                                      CodeT handler) {
    DCHECK(handler.is_off_heap_trampoline());
    DCHECK(handler.kind() == CodeKind::BYTECODE_HANDLER);
    size_t index = GetDispatchTableIndex(bytecode, operand_scale);
    dispatch_table_[index] = handler.InstructionStart();
  }
  ```

The bytecode array also contains something called a **“Constant Pool Pointer”** which stores heap objects that are referenced as constants in generated bytecode, such as strings and integers. The constant pool is a FixedArray of pointers to heap objects. An example of this BytecodeArray pointer and its constant pool of heap objects can be seen below.

> Question: what is the difference between bytecode in SharedFunctionInfo and bytecode in BytecodeArray?
> Answer: they are the same!

### Understanding V8’s Bytecode

```js
d8> incX({x:13});
...
[generated bytecode for function: incX (0x026c0025ab65 <SharedFunctionInfo incX>)]
Bytecode length: 11
Parameter count 2
Register count 1
Frame size 8
Bytecode age: 0
         0000026C0025ACC6 @    0 : 0d 01             LdaSmi [1]
         0000026C0025ACC8 @    2 : c5                Star0
         0000026C0025ACC9 @    3 : 2d 03 00 01       GetNamedProperty a0, [0], [1]
         0000026C0025ACCD @    7 : 39 fa 00          Add r0, [0]
         0000026C0025ACD0 @   10 : aa                Return
Constant pool (size = 1)
0000026C0025AC99: [FixedArray] in OldSpace
 - map: 0x026c00002231 <Map(FIXED_ARRAY_TYPE)>
 - length: 1
           0: 0x026c000041ed <String[1]: #x>
Handler Table (size = 0)
Source Position Table (size = 0)
14
```

The GetNameProperty bytecode gets a named property from a0 and stores it in the accumulator, which will be the value of 13. 

The other `[1]` operand is known as a feedback vector which contains runtime information and object shape data that is used for optimization by TurboFan.

# Sparkplug the Baseline Compiler

Sparkplug doesn’t produce any intermediate representation (IR) like most compilers do (which we’ll learn about later). In this case, Sparkplug compiles directly to machine code in a **single linear pass** over the bytecode. This in general is known as 1:1 mapping.

The funny thing is that Sparkplug is pretty much just a switch statement inside a for loop which dispatches to fixed bytecode and then generates the machine code.
- This is the switch statement which inside the `BaselineCompiler::VisitSingleBytecode()` function:

  ```cpp
      switch (bytecode) {
  #define BYTECODE_CASE(name, ...)       \
    case interpreter::Bytecode::k##name: \
      Visit##name();                     \
      break;
        BYTECODE_LIST(BYTECODE_CASE)
  #undef BYTECODE_CASE
      }
  // SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/baseline/baseline-compiler.cc;l=518;bpv=1;bpt=1
  ```

- And this is the loop that call that function which inside the `BaselineCompiler::GenerateCode()` function:

  ```cpp
  for (; !iterator_.done(); iterator_.Advance()) {
    VisitSingleBytecode();
    AddPosition();
  }
  ```

## Generating Machine Code

Sparkplug pretty much just **serializes the interpreter’s execution by calling the same built-ins** that are usually entered by the `InterpreterEntryTrampoline` and then handled within v8/src/builtins/x64/builtins-x64.cc. This allows for the JavaScript function to be faster, because by doing this we can avoid the interpreter overheads like opcode decoding and bytecode dispatching lookups.

If you remember back to our JSFunction object during our talk about Ignition, you’ll remember that the closure linked to “optimized code”. In essence, Sparkplug will store the bytecode’s built-in there.

> Question: what is Sparklug built-ins and how does it look like?

## 1:1 Mapping

For Sparkplug to be fast and to avoid having to do any register allocation of its own, it reuses Ignitions register frames which in turn allows Sparkplug to mirror the interpreter’s behavior and stack as well.

There is one small difference between the Ignition and Sparkplug stack frames. And that difference is that Sparkplug doesn’t need to keep the bytecode offset slot in the register file since Sparkplug code is emitted directly from the bytecode. Instead, it replaces it with the cached feedback vector.

So why does Sparkplug need to creates and maintains a stack frame layout that’s similar to Ignitions? For one reason, and for the main reason of how Sparkplug and Turbofan work, by doing something called **on-stack replacement (OSR)**. OSR is the ability to replace currently executing code with a different version. In this case, when Ignition sees that a JavaScript function is used a lot, it will send it to Sparkplug to speed it up by replacing with the built-in version.

Once Sparkplug serializes the bytecodes to their builtins, it will replace the Interpreters stack frame for that specific function. When the stack is walked and executed, the **code will jump directly into Sparkplug instead of being executed on Ignitions** emulated stack. And since the frames are “mirrored”, this technically allows V8 to swap between the interpreter and Sparkplug code with almost zero frame translation overhead.

The bigger security risk with Sparkplug is with how the layout of Ignitions stack frames are interpreted, which can lead to a type confusion or code execution on the stack.

# TurboFan the Optimizing Compiler

## JIT Compilation

If the function is used a lot of times and the general shape of the object doesn’t change, V8 might decide that it’s better to just pass it up the pipeline (known as a “tier-up”) for compilation so that it’s executed faster.

As you can see in line one of the optimization trace below, we are marking the JSFunction’s SFI or `SharedFunctionInfo` for optimization. If you remember back to our Ignition deep dive, you’ll remember that the SFI contains the bytecode for our function. TurboFan will use that bytecode to generate IR and then optimize it down to machine code.

```js
d8> function hot_function(obj) {return obj.x;}
undefined
d8> for (let i=0; i < 10000; i++) {hot_function({x:i});}
[marking 0x12590025b7e1 <JSFunction (sfi = 0x12590025b70d)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[compiling method 0x12590025b7e1 <JSFunction (sfi = 0x12590025b70d)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x12590025b7e1 <JSFunction (sfi = 0x12590025b70d)> (target TURBOFAN) OSR - took 0.941, 12.404, 0.218 ms]
[completed optimizing 0x12590025b7e1 <JSFunction (sfi = 0x12590025b70d)> (target TURBOFAN) OSR]
```

Pretty much TurboFan does the same thing Sparkplug does when it optimizes bytecode. It will **replace the stack frame with a real JIT or system stack frame** that will point to the optimized code during runtime. This allows the function to go directly to the optimized code the next time it is called, versus being executed within Ignitions emulated stack.

To those with a keen eye, you might have noticed something interesting when we traced the optimization of our function. If you paid close attention, you would have noticed that TurboFan didn’t kick in right away, but after a few seconds - or after a few thousand iterations of the loop. Why is that?

```js
d8> for (let i=0; i < 10000; i++) {hot_function({x:i})}
[marking 0x3ee60025c3e5 <JSFunction (sfi = 0x3ee60025bf59)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[compiling method 0x3ee60025c3e5 <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x3ee60025c3e5 <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR - took 0.017, 1.531, 0.059 ms]
[completed optimizing 0x3ee60025c3e5 <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR]
9999

d8> for (let i=0; i < 10000; i++) {hot_function({x:i})}
[marking 0x3ee60025c661 <JSFunction (sfi = 0x3ee60025bf59)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[compiling method 0x3ee60025c661 <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x3ee60025c661 <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR - took 0.017, 1.572, 0.049 ms]
[completed optimizing 0x3ee60025c661 <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR]
9999

d8> for (let i=0; i < 10000; i++) {hot_function({x:i})}
[marking 0x3ee60025c8bd <JSFunction (sfi = 0x3ee60025bf59)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[marking 0x3ee60025a481 <JSFunction hot_function (sfi = 0x3ee60025a3bd)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[compiling method 0x3ee60025a481 <JSFunction hot_function (sfi = 0x3ee60025a3bd)> (target TURBOFAN), mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x3ee60025a481 <JSFunction hot_function (sfi = 0x3ee60025a3bd)> (target TURBOFAN) - took 0.018, 0.642, 0.048 ms]
[completed optimizing 0x3ee60025a481 <JSFunction hot_function (sfi = 0x3ee60025a3bd)> (target TURBOFAN)]
[compiling method 0x3ee60025c8bd <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x3ee60025c8bd <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR - took 0.010, 1.345, 0.040 ms]
[completed optimizing 0x3ee60025c8bd <JSFunction (sfi = 0x3ee60025bf59)> (target TURBOFAN) OSR]
```

**TurboFan waits for the code to “warm up”**. If you remember back to our discussion about Ignition and Sparkplug, we briefly mentioned the feedback vector. This vector stores the object runtime data along with information from the inline caches and collects what is known as type feedback.

```js
d8> %DisassembleFunction(hot_function)
0x12590025bcd5: [CodeDataContainer] in OldSpace
 - map: 0x125900003d7d <Map[24](CODE_DATA_CONTAINER_TYPE)>
 - kind: TURBOFAN
 - is_off_heap_trampoline: 0
 - code: 0x73b300004241 <Code TURBOFAN>
 - code_entry_point: 0x73b300004280
 - kind_specific_flags: 4
```

## Speculative Optimization

Within the feedback vector, there is an interesting slot called the `BinaryOp` slot, which records feedback about the inputs and outputs of binary operations such as +, -, *, etc:

```js
d8> function add(i) {return 1 + i;}
d8> for (let i=0; i<100; i++) {add(i);}
d8> %DebugPrint(add)
DebugPrint: 0000019A002596F1: [Function] in OldSpace
 - ...
 - feedback vector: 0000019A0025B759: [FeedbackVector] in OldSpace
 - map: 0x019a0000273d <Map(FEEDBACK_VECTOR_TYPE)>
 - length: 1
 - shared function info: 0x019a0025962d <SharedFunctionInfo add>
 - no optimized code
 - tiering state: TieringState::kNone
 - maybe has maglev code: 0
 - maybe has turbofan code: 0
 - invocation count: 97
 - profiler ticks: 0
 - closure feedback cell array: 0000019A00003511: [ClosureFeedbackCellArray] in ReadOnlySpace
 - map: 0x019a00002981 <Map(CLOSURE_FEEDBACK_CELL_ARRAY_TYPE)>
 - length: 0
 - ...
 - slot #0 BinaryOp BinaryOp:SignedSmall {
     [0]: 1
  }
```

Invocation count shows us the number of times we ran the add function, and if we look into our feedback vector, you’ll see that we have exactly one slot, which is the `BinaryOp` that we talked about. Looking into that slot we see that it contains the current feedback type of `SignedSmall` which in essence refers to an SMI.

Overall, these speculations via feedback vectors are great in helping speed up our code by **removing unnecessary machine instructions for different types**.

## Type Guards

To protect against potentially wrong assumptions, TurboFan prepends something known as a type guard before execution of specific instructions.

This type guard checks to make sure that the shape of the object we are passing in is the correct type. This is done before the object reaches our optimized operations. If the object does not match the expected shape, then the execution of the optimized code can’t continue. In that case, we will “bail out” of the assembly code, and jump back to the unoptimized bytecode within the interpreter and continue execution there. This is known as “deoptimization”.

The assembly code for a type guard looks like this:

```s
REX.W movq rcx,[rbp-0x38]       ; Move i to rcx
testb rcx,0x1                   ; Check if rcx is an SMI
jnz 00007FFB0000422A  <+0x1ea>  ; If check fails, bailout
```

Now deoptimizations due to type guards aren’t just limited in checking if there is a mismatch in object types. They **also work on arithmetic operations and bound checking**.

For example, if our optimized code was optimized for arithmetic operations on 32bit integers, and there is an overflow, we can deoptimize and let Ignition handle the calculations - thus protecting us from potential security issues on the machine. **Such issues that can lead to deoptimization are known as “side-effects”** (which we’ll cover in more detail later).

What if we use another type that is different from the expected type while the function is optimized? Let’s try it out:

```js
for (let i=0; i<10000; i++) {
	if (i<7000) {
		add(i);
	} else {
		add("string");
	}
}
```

Let’s see what happens:

```js
./v8-11.1/d8 --allow-natives-syntax --trace-opt --trace-deopt deopt.js
[marking 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[compiling method 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> (target TURBOFAN) OSR - took 0.060, 2.523, 0.197 ms]
[completed optimizing 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> (target TURBOFAN) OSR]
[bailout (kind: deopt-eager, reason: Insufficient type feedback for call): begin. deoptimizing 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)>, 0x78d880004001 <Code TURBOFAN>, opt id 0, node id 63, bytecode offset 51, deopt exit 3, FP to SP delta 96, caller SP 0x7ffd48fc7ed0, pc 0x78d880004274]
[compiling method 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> (target TURBOFAN) OSR - took 0.032, 1.917, 0.038 ms]
[completed optimizing 0x14e40025a4b1 <JSFunction (sfi = 0x14e40025a33d)> (target TURBOFAN) OSR]
```

As you can see, the functions gets optimized, and later we trigger a bailout. This deoptimizes the code back to bytecode due to an insufficient type during our call. Then, something interesting happens. The function gets optimized again.

Well, the function is still “hot” and there are a few more thousand iterations to go. What TurboFan will do, now that it collected both a number and string within the type feedback, is that it will go back and optimize the code for a second time. But this time it will add code which will allow for string evaluation. In this case, a second type guard will be added - so the second run of code is now optimized for both a number and a string!

For example, take this code:

```js
function add(i) {return 1 + i;}
for (let i=0; i<10000; i++) {
	if (i < 7000 || i >= 9000) {
		add(i);
	} else {
		add("string");
	}
	// if (i == 6999)
	// 	%DebugPrint(add);
	// if (i == 7000)
	// 	%DebugPrint(add);
	// if (i == 9000)
	// 	%DebugPrint(add);
};
```

The trace will be:

```js
./v8-11.1/d8 --allow-natives-syntax --trace-opt --trace-deopt deopt.js
[marking 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> for optimization to TURBOFAN, ConcurrencyMode::kConcurrent, reason: small function]
[compiling method 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> (target TURBOFAN) OSR - took 0.099, 3.083, 0.355 ms]
[completed optimizing 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> (target TURBOFAN) OSR]
[bailout (kind: deopt-eager, reason: Insufficient type feedback for compare operation): begin. deoptimizing 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)>, 0x761b20004001 <Code TURBOFAN>, opt id 0, node id 47, bytecode offset 19, deopt exit 3, FP to SP delta 96, caller SP 0x7ffe738497a0, pc 0x761b20004274]
[compiling method 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> (target TURBOFAN) OSR, mode: ConcurrencyMode::kConcurrent]
[completed compiling 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> (target TURBOFAN) OSR - took 0.043, 2.561, 0.065 ms]
[completed optimizing 0x3eab0025a4d5 <JSFunction (sfi = 0x3eab0025a33d)> (target TURBOFAN) OSR]
```

As we can see, there are just one deoptimization and reoptimization because the type guard for both number and string is added in the reoptimized code.

The feedback vector after the first optimization:

```js
 - slot #0 BinaryOp BinaryOp:Any {
     [0]: 127
  }
```

As you can see, the BinaryOp now stores the feedback type of `Any`, instead of `SignedSmall` and `String`. Why? Well, this is due to something called the Feedback Lattice.

## Feedback Lattice

The feedback **lattice stores the possible feedback states for an operation**. It starts with None, which indicates that it hasn’t seen anything and it goes down toward the Any state, which indicates that it’s seen a combination of inputs and outputs.

The Any state indicates that the function is to be considered polymorphic, while in contrast, any other state indicates that the function is monomorphic - since it’s only produced a certain value.

Feedback can only progress downwards in the lattice. Once we go from Number to Any, we can never go back. If we do go back for some magic reason, then we risk entering a so-called deoptimization loop where the optimizing compiler consumes invalid feedback and bails out from optimized code continuously.

All of the type check kinds:

```cpp
enum class TypeCheckKind : uint8_t {
  kNone,
  kSignedSmall,
  kSigned32,
  kSigned64,
  kNumber,
  kNumberOrBoolean,
  kNumberOrOddball,
  kHeapObject,
  kBigInt,
  kBigInt64,
  kArrayIndex
};
// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/use-info.h;l=124;bpv=0;bpt=1
```

## “Sea of Nodes” Intermediate Representation (IR)

TurboFans “Sea of Nodes” IR is based on static single assignment or SSA, which is a property of IR that requires each variable to be assigned exactly once and defined before it is used. This is useful for optimizations such as redundancy elimination.

```js
// function add(i) {return 1 + i;}
var i1 = argument
var r1 = 1 + i1
return r1
```

This SSA form is then converted to a graph format, which is similar to a control-flow graph (CFG) where it uses nodes and edges to represent code and its dependencies between computations.

Take this for example:

```js
function hot_function(obj) {
	return obj.x;
}

for (let i=0; i < 10000; i++) {
	hot_function({x:i});
}
```

The generated IR for this function will look like this:

![sea-of-nodes](sea-of-nodes.png)

If we follow the Branch node up, we will see that it has a SpeculativeNumberLessThan node which has a value edge pointing to a NumberConstant with the value of 10000. This falls in line with our function, since we were looping 10k times. Since this node is Green, it is a machine instruction, and signifies our type guard for the loop.

For those that might be wondering what the Phi node is, that’s basically an SSA node that merges the two (or more) possibilities for a value that have been computed by different branches. In this case it’s merging both of the potential integer speculations together.

> Question: still don't get the Phi node, can you explain it more? Answer: https://chatgpt.com/c/421ac553-9c20-4aad-9da0-b4057e2a479a

# Common Optimizations

From here we can dive into understanding some of TurboFans common optimizations. These optimizations in essence act on the original graph that was produce from bytecode.

Since the resulting graph now has static type information due to type guards, the optimizations are done in a more classic ahead-of-time fashion to improve the execution speed or memory footprint of the code.

Afterwards, once the graph is optimized, the resulting graph is lowered to machine code (known as “lowering”) and is then written into an executable memory region for V8 to execute when the compiled function is called.

Lowering can happen in multiple stages with further optimizations in between, making this compiler pipeline pretty flexible.

## Typer

One of the earliest optimization phases is called the `TyperPhase` which is ran by the `OptimizeTurbofanGraph` function. This phase traces through the code and identifies the resulting types of operations from heap objects, such as Int32 + Int32 = Int32.

`OptimizeTurbofanGraph` is a function that runs the `TyperPhase`, along with other phases:

```cpp
bool PipelineImpl::OptimizeTurbofanGraph(Linkage* linkage) {
  DCHECK(!v8_flags.turboshaft_from_maglev);
  TFPipelineData* data = this->data_;

  data->BeginPhaseKind("V8.TFLowering");

  // Trim the graph before typing to ensure all nodes are typed.
  Run<EarlyGraphTrimmingPhase>();
  RunPrintAndVerify(EarlyGraphTrimmingPhase::phase_name(), true);

  // Type the graph and keep the Typer running such that new nodes get
  // automatically typed when they are created.
  Run<TyperPhase>(data->CreateTyper());
  RunPrintAndVerify(TyperPhase::phase_name());

  Run<TypedLoweringPhase>();
  RunPrintAndVerify(TypedLoweringPhase::phase_name());

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/pipeline.cc;l=2539;drc=90cac1911508d3d682a67c97aa62483eb712f69a;bpv=0;bpt=1
```

`TyperPhase` is just a struct with a `Run` function, which will call the `Typer::Run` function:

```cpp
struct TyperPhase {
  void Run(PipelineData* data, Zone* temp_zone, Typer* typer) {
    // [...]
    typer->Run(roots, &induction_vars);
  }
};
// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/pipeline.cc;l=1084;drc=90cac1911508d3d682a67c97aa62483eb712f69a;bpv=0;bpt=1
```

The `Typer::Run` function will visit every node of the graph and will try to “reduce” them down by trying to simplify operation logic. It will then call the node’s associated typer call to associate a `Type` with it:

```cpp
void Typer::Run(const NodeVector& roots,
                LoopVariableOptimizer* induction_vars) {
  // [...]
  Visitor visitor(this, induction_vars);
  GraphReducer graph_reducer(zone(), graph());
  graph_reducer.AddReducer(&visitor);
  for (Node* const root : roots) graph_reducer.ReduceNode(root);
  graph_reducer.ReduceGraph();
  // [...]
}

class Typer::Visitor : public Reducer {
// ...
  Reduction Reduce(Node* node) override {
// calls visitors such as JSCallTyper
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/typer.cc;drc=90cac1911508d3d682a67c97aa62483eb712f69a;bpv=0;bpt=1;l=471
```

For example, in our case the constant integers within the loop and return arithmetic will be visited by `Typer::Visitor::TypeNumberConstant`, which will return a type of `Range`.

Now what about our speculation nodes?

For those, they are handled by the `OperationTyper`. In our case, the arithmetic speculation for returning our value will call `OperationTyper::SpeculativeSafeIntegerAdd` which will set the type to a “safe integer” range, such as `Int64`. This type will be checked, and if it’s not an Int64 during execution, we deoptimize.

```cpp
#define SPECULATIVE_NUMBER_BINOP(Name)                         \
  Type OperationTyper::Speculative##Name(Type lhs, Type rhs) { \
    lhs = SpeculativeToNumber(lhs);                            \
    rhs = SpeculativeToNumber(rhs);                            \
    return Name(lhs, rhs);                                     \
  }
SPECULATIVE_NUMBER_BINOP(NumberAdd)
#undef SPECULATIVE_NUMBER_BINOP

Type OperationTyper::SpeculativeToNumber(Type type) {
  return ToNumber(Type::Intersect(type, Type::NumberOrOddball(), zone()));
}

// SOURCE https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/operation-typer.cc;l=708
```

## Range Analysis

During the `Typer` optimization the compiler traces through the code, identifies the range of operations and calculates the bounds of the resulting values. This is known as range analysis.

Range analysis optimizer computes the min and max of values that are added or returned.

In our case, we were returning the value of i from our object’s property of x plus 1. The type feedback only really knew that the value returned is an integer and that’s it, it never really could tell what range the value would be. So, to err on the safe side, it decided to give it the largest value possible in order to prevent issues.

```js 
function hot_function(obj) {
	let values = [0,13,1337]
	let a = 1;
	if (obj == "leet")
		a = 2;
	return values[a];
}
```

As we can see, depending on what type of obj parameter is passed in, if obj is a string that equals the word leet then a will equal 1337, otherwise it will equal 13. This part of the code will go through SSA and be merged into a Phi node that will contain the range of what a can be. The constants will have their range set to their hardcoded value, but these constants will also have an effect on our speculative ranges due to arithmetic computations.

As you can see, due to SSA we have the Phi node. During range analysis the typer visits the TypePhi node function and creates a union of the operands 13 and 1337, allowing us to have the possible range for a.

```cpp
Type Typer::Visitor::TypePhi(Node* node) {
  int arity = node->op()->ValueInputCount();
  Type type = Operand(node, 0);
  for (int i = 1; i < arity; ++i) {
    type = Type::Union(type, Operand(node, i), zone());
  }
  return type;
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/typer.cc;l=927;bpv=0;bpt=1
```

To understand the typing of the `SpeculativeSafeIntegerAdd` nodes, we need to go back to the `OperationTyper` implementation. In the case of `SpeculativeSafeIntegerAdd(n,m)`, TurboFan does an `AddRanger(n.Min(), n.Max(), m.Min(), m.Max())`.

`AddRanger` is the function that actually computes the min and max bounds of the Range.

```cpp
Type OperationTyper::AddRanger(double lhs_min, double lhs_max, double rhs_min,
                               double rhs_max) {
  double results[4];
  results[0] = lhs_min + rhs_min;
  results[1] = lhs_min + rhs_max;
  results[2] = lhs_max + rhs_min;
  results[3] = lhs_max + rhs_max;
  // Since none of the inputs can be -0, the result cannot be -0 either.
  // However, it can be nan (the sum of two infinities of opposite sign).
  // On the other hand, if none of the "results" above is nan, then the
  // actual result cannot be nan either.
  int nans = 0;
  for (int i = 0; i < 4; ++i) {
    if (std::isnan(results[i])) ++nans;
  }
  if (nans == 4) return Type::NaN();
  Type type = Type::Range(array_min(results, 4), array_max(results, 4), zone());
  if (nans > 0) type = Type::Union(type, Type::NaN(), zone());
  // Examples:
  //   [-inf, -inf] + [+inf, +inf] = NaN
  //   [-inf, -inf] + [n, +inf] = [-inf, -inf] \/ NaN
  //   [-inf, +inf] + [n, +inf] = [-inf, +inf] \/ NaN
  //   [-inf, m] + [n, +inf] = [-inf, +inf] \/ NaN
  return type;
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/operation-typer.cc;l=165
```

In this case you can see that the typer computes the range of our return values for both possible iteration of a after our arithmetic operations. With this, in the case that the range analysis fails and we get a value not expected by the compiler, we deoptimize. Pretty simple to understand!

## Bounds Checking Elimination (BCE)

Another common optimization that was applied with the Typer during the simplified lowering phase was the CheckBounds operation which is applied to CheckBound speculative nodes. This optimization is usually applied to array access operations if the index of the array has been proven to be within the bounds of the array after range analysis.

```cpp
Type OperationTyper::CheckBounds(Type index, Type length) {
  DCHECK(length.Is(cache_->kPositiveSafeInteger));
  if (length.Is(cache_->kSingletonZero)) return Type::None();
  Type const upper_bound = Type::Range(0.0, length.Max() - 1, zone());
  if (index.Maybe(Type::String())) return upper_bound;
  if (index.Maybe(Type::MinusZero())) {
    index = Type::Union(index, cache_->kSingletonZero, zone());
  }
  return Type::Intersect(index, upper_bound, zone());
}

// SOURCE: 
```

Example:

```js
function hot_function(obj) {
	let values = [0,13,1337]
	let a = 1;
	if (obj == "leet")
		a = 2;
	return values[a];
}

for (let i=0; i < 10000; i++) {
	hot_function({x:i});
}
```

Particularly take note that a never equals 0, so we’ll never or at least should never be able to return 0.

As you can see, we have another Phi node that merges our potential values of a and then we have our `CheckBounds` node which is used to check the bounds of the array. If we are in range of 1 or 2, we call `LoadElement` to load our element from the array, otherwise we will bailout since the bounds check is not expecting an index of 0.

![bound-checking](bound-checking.png)

For those that have noticed it already, you might be wondering why our LoadElement is of type `Signed31` and not a `Signed32`. Simply, `Signed31` represents the fact that the first bit is used to denote sign. This means that, in the case of a 32-bit signed integer, we are actually working with 31 value bits instead of 32. Also, as we can see the `LoadElement` has an input of a `FixedArray` HeapConstant with a length of 3. This array would be our values array.

Once escape analysis has been conducted, we move onto the simplified lowering phase. This lowering phase simply (pun intended) changes all value representations to the correct machine representation, as dictated by the machine operators themselves.

Some of the machine representation:

```cpp
MachineRepresentation MachineRepresentationFromArrayType(
    ExternalArrayType array_type) {
  switch (array_type) {
    case kExternalUint8Array:
    case kExternalUint8ClampedArray:
    case kExternalInt8Array:
      return MachineRepresentation::kWord8;
    case kExternalUint16Array:
    case kExternalInt16Array:
      return MachineRepresentation::kWord16;
    case kExternalUint32Array:
    case kExternalInt32Array:
      return MachineRepresentation::kWord32;
    case kExternalFloat32Array:
      return MachineRepresentation::kFloat32;
    case kExternalFloat64Array:
      return MachineRepresentation::kFloat64;
    case kExternalBigInt64Array:
    case kExternalBigUint64Array:
      return MachineRepresentation::kWord64;
    case kExternalFloat16Array:
      UNIMPLEMENTED();
  }
  UNREACHABLE();
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/simplified-lowering.cc;l=80;bpv=1;bpt=1?q=MachineRepresentationFromArrayType&ss=chromium%2Fchromium%2Fsrc
```

For each CheckBounds node the `VisitCheckBounds` function is going to be called. This function is responsible for checking and making sure that the index’s minimum range is equal to or greater than zero and that it’s maximum range does not exceed the array length. If the check is true, then it triggers a DeferReplacement which marks the node for removal.

A part of `VisitCheckBounds`:

```cpp
void VisitCheckBounds(Node* node, SimplifiedLowering* lowering) {
  // ...
      if (lower<T>()) {
        if (index_type.IsNone() || length_type.IsNone() ||
            (index_type.Min() >= 0.0 &&
              index_type.Max() < length_type.Min())) {
          // The bounds check is redundant if we already know that
          // the index is within the bounds of [0.0, length[.
          // TODO(neis): Move this into TypedOptimization?
          if (v8_flags.turbo_typer_hardening) {
            new_flags |= CheckBoundsFlag::kAbortOnOutOfBounds;
          } else {
            DeferReplacement(node, NodeProperties::GetValueInput(node, 0));
            return;
          }
        }
        ChangeOp(node,
                  simplified()->CheckedUint32Bounds(feedback, new_flags));
      }

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/simplified-lowering.cc;l=1868;bpv=0;bpt=1
```

As you can see our `CheckBound` range would fall into the if statement, where `Range(1,2).Min() >= 0` and `Range(1,2).Max() < 3`. In that case our node #46 from the above graph would be made redundant and removed.

In case `v8_flags.turbo_typer_hardening` flag is enbled, the `CheckBounds` will be replaced with `CheckedUint32Bounds` node:

![simplified-lowering](simplified-lowering.png)


If we look at the simplified lowering portion of the graph, we can indeed see that the CheckBounds node has now been replaced with a CheckedUint32Bounds node as per the code and all other nodes had their values “lowered” to machine code representation.

## Redundancy Elimination

The `RedundancyElimination` class is essentially a graph reducer that tries to either remove or combine redundant checks in the effect chain.

The effect chain is pretty much the order of operations between effect edges for load and store functions. For example, if we try to load a property from an object and try to add to it, such as obj[x] = obj[x] + 1 then our effect chain would be JSLoadNamed => SpeculativeSafeIntegerAdd => JSStoreNamed. TurboFan has to make sure that these nodes external effects aren’t reordered or otherwise we might have improper guards in place.

One of the reduce functions:

```cpp
Reduction RedundancyElimination::ReduceCheckNode(Node* node) {
  Node* const effect = NodeProperties::GetEffectInput(node);
  EffectPathChecks const* checks = node_checks_.Get(effect);
  // If we do not know anything about the predecessor, do not propagate just yet
  // because we will have to recompute anyway once we compute the predecessor.
  if (checks == nullptr) return NoChange();
  // See if we have another check that dominates us.
  if (Node* check = checks->LookupCheck(node, jsgraph_)) {
    ReplaceWithValue(node, check);
    return Replace(check);
  }

  // Learn from this check.
  return UpdateChecks(node, checks->AddCheck(zone(), node));
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/redundancy-elimination.cc;drc=90cac1911508d3d682a67c97aa62483eb712f69a;l=338
```

A reducer, as detailed in v8/src/compiler/graph-reducer.h, tries to simplify a given node based on its operator and inputs. There are few type of reducers such as constant folding, where if we add two constants with each other, we’ll fold them over to just one, i.e. 3 + 5 will now just be a single constant node of 8, and strength reduction where if a value is added to a node with no effects, we’ll keep a single node, i.e. x + 0 will just have the node x.

The reduce function of constant folding reducer:

```cpp
Reduction ConstantFoldingReducer::Reduce(Node* node) {
  if (!NodeProperties::IsConstant(node) && NodeProperties::IsTyped(node) &&
      node->op()->HasProperty(Operator::kEliminatable) &&
      node->opcode() != IrOpcode::kFinishRegion &&
      node->opcode() != IrOpcode::kTypeGuard) {
    Node* constant = TryGetConstant(jsgraph(), node, broker());
    if (constant != nullptr) {
      DCHECK(NodeProperties::IsTyped(constant));
      DCHECK_EQ(node->op()->ControlOutputCount(), 0);
      ReplaceWithValue(node, constant);
      return Replace(constant);
    }
  }
  return NoChange();
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/constant-folding-reducer.cc;l=50
```


The `TryGetConstant` function:

```cpp
Node* TryGetConstant(JSGraph* jsgraph, Node* node, JSHeapBroker* broker) {
  Type type = NodeProperties::GetType(node);
  Node* result;
  if (type.IsNone()) {
    result = nullptr;
  } else if (type.Is(Type::Null())) {
    result = jsgraph->NullConstant();
  } else if (type.Is(Type::Undefined())) {
    result = jsgraph->UndefinedConstant();
  } else if (type.Is(Type::MinusZero())) {
    result = jsgraph->MinusZeroConstant();
  } else if (type.Is(Type::NaN())) {
    result = jsgraph->NaNConstant();
  } else if (type.IsHeapConstant()) {
    result = jsgraph->ConstantNoHole(type.AsHeapConstant()->Ref(), broker);
  } else if (type.Is(Type::PlainNumber()) && type.Min() == type.Max()) {
    result = jsgraph->ConstantNoHole(type.Min());
  } else {
    result = nullptr;
  }
  DCHECK_EQ(result != nullptr, type.IsSingleton());
  DCHECK_IMPLIES(result != nullptr,
                 type.Equals(NodeProperties::GetType(result)));
  return result;
}

// SOURCE: https://source.chromium.org/chromium/chromium/src/+/main:v8/src/compiler/constant-folding-reducer.cc;drc=90cac1911508d3d682a67c97aa62483eb712f69a;l=16
```

The reason for this is that certain operation can cause side-effects to the observable execution of our context - that’s why we have side effect chains. If TurboFan for some reason forgot to take into account a side-effect and doesn’t write it to the side-effect chain, then it’s possible that the Map of the object can change during execution, such as another user function call modifying the object or adding a property.

Each intermediate representation operation in V8 has various flags associated with it. An example of a few of the flags for JavaScript operators can be seen in v8/src/compiler/js-operator.cc Some of these flags have specific assumptions around them.

For example, V(ToString, Operator::kNoProperties, 1, 1) assumes that a String should have no properties. Another one such as V(LoadMessage, Operator::kNoThrow | Operator::kNoWrite, 0, 1) assumes that the LoadMessage operation will not have observable side-effects via the kNoWrite flag. This kNoWrite flag does not actually write to the effect chain.

If we can get the compiler to remove a redundancy check for an operation that seemingly thinks there are no side-effects, then you have a potentially exploitable bug if you can modify an object or property during the execution of the compiled code.

# Common JIT Compiler Vulnerabilities

Logic bugs can stem from the incorrect assumption of the potential side-effects an operation can have on an object or property or from improper optimization passes which remove critical type guards.

These types of issues are generally known as “type-confusion” vulnerabilities where the compiler doesn’t verify the type or shape of the object that is passed to it, resulting in the compiler blindly using the object.